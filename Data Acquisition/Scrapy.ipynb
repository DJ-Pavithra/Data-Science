{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "zT515cdWIxga",
        "outputId": "e627eb6f-3f5a-4f84-8836-ab5bc2bdcaa5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapy\n",
            "  Downloading scrapy-2.13.3-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: cryptography>=37.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (43.0.3)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from scrapy) (0.7.1)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.11.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (5.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scrapy) (25.0)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading protego-0.5.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting pydispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: pyopenssl>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from scrapy) (24.2.1)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.8.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting twisted>=21.7.0 (from scrapy)\n",
            "  Downloading twisted-25.5.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting zope-interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (25.3.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.11/dist-packages (from service-identity>=18.1.0->scrapy) (0.4.2)\n",
            "Collecting automat>=24.8.0 (from twisted>=21.7.0->scrapy)\n",
            "  Downloading automat-25.4.16-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting constantly>=15.1 (from twisted>=21.7.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting hyperlink>=17.1.1 (from twisted>=21.7.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting incremental>=24.7.0 (from twisted>=21.7.0->scrapy)\n",
            "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from twisted>=21.7.0->scrapy) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope-interface>=5.1.0->scrapy) (75.2.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.10)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (2.32.3)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->scrapy) (3.18.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2025.7.14)\n",
            "Downloading scrapy-2.13.3-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading itemadapter-0.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
            "Downloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\n",
            "Downloading protego-0.5.0-py3-none-any.whl (10 kB)\n",
            "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Downloading queuelib-1.8.0-py3-none-any.whl (13 kB)\n",
            "Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading twisted-25.5.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.0-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading automat-25.4.16-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: pydispatcher, zope-interface, w3lib, queuelib, protego, jmespath, itemadapter, incremental, hyperlink, cssselect, constantly, automat, twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed automat-25.4.16 constantly-23.10.4 cssselect-1.3.0 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.11.0 itemloaders-1.3.2 jmespath-1.0.1 parsel-1.10.0 protego-0.5.0 pydispatcher-2.0.7 queuelib-1.8.0 requests-file-2.1.0 scrapy-2.13.3 service-identity-24.2.0 tldextract-5.3.0 twisted-25.5.0 w3lib-2.3.1 zope-interface-7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!scrapy startproject myproject"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4D4w-WDEN9Lc",
        "outputId": "ba0b6181-4af6-4551-e94b-8805c22f1666"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Scrapy project 'myproject', using template directory '/usr/local/lib/python3.11/dist-packages/scrapy/templates/project', created in:\n",
            "    /content/myproject\n",
            "\n",
            "You can start your first spider with:\n",
            "    cd myproject\n",
            "    scrapy genspider example example.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd myproject && ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeC8BQHd7s2l",
        "outputId": "945ed3a8-59e0-440b-d34b-0e5ba91eac18"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "myproject  scrapy.cfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spider_code = '''\n",
        "import scrapy\n",
        "\n",
        "class QuotesSpider(scrapy.Spider):\n",
        "    name = \"quotes_spider\"\n",
        "\n",
        "    def start_requests(self):\n",
        "        urls = [\n",
        "            'http://quotes.toscrape.com/page/1/',\n",
        "            'http://quotes.toscrape.com/page/2/',\n",
        "        ]\n",
        "        for url in urls:\n",
        "            yield scrapy.Request(url=url, callback=self.parse)\n",
        "\n",
        "    def parse(self, response):\n",
        "        page_id = response.url.split(\"/\")[-2]\n",
        "        filename = f\"quotes-{page_id}.html\"\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(response.body)\n",
        "        self.log(f\"Saved file {filename}\")\n",
        "'''\n",
        "with open('myproject/myproject/spiders/quotes_spider.py', 'w') as f:\n",
        "    f.write(spider_code)\n"
      ],
      "metadata": {
        "id": "MazYbAxy8O7F"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd myproject && scrapy crawl quotes_spider"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "tyOBLBfu8TFL",
        "outputId": "dfcc6ee9-2961-4808-9f19-0d8a68369c9d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-23 15:41:11 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: myproject)\n",
            "2025-07-23 15:41:11 [scrapy.utils.log] INFO: Versions:\n",
            "{'lxml': '5.4.0',\n",
            " 'libxml2': '2.13.8',\n",
            " 'cssselect': '1.3.0',\n",
            " 'parsel': '1.10.0',\n",
            " 'w3lib': '2.3.1',\n",
            " 'Twisted': '25.5.0',\n",
            " 'Python': '3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]',\n",
            " 'pyOpenSSL': '24.2.1 (OpenSSL 3.3.2 3 Sep 2024)',\n",
            " 'cryptography': '43.0.3',\n",
            " 'Platform': 'Linux-6.1.123+-x86_64-with-glibc2.35'}\n",
            "2025-07-23 15:41:11 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2025-07-23 15:41:11 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2025-07-23 15:41:11 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-07-23 15:41:11 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-07-23 15:41:11 [scrapy.extensions.telnet] INFO: Telnet Password: c426f511c4eda06f\n",
            "2025-07-23 15:41:11 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2025-07-23 15:41:11 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'myproject',\n",
            " 'CONCURRENT_REQUESTS_PER_DOMAIN': 1,\n",
            " 'DOWNLOAD_DELAY': 1,\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'myproject.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['myproject.spiders']}\n",
            "2025-07-23 15:41:12 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2025-07-23 15:41:12 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
            " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2025-07-23 15:41:12 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2025-07-23 15:41:12 [scrapy.core.engine] INFO: Spider opened\n",
            "/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py:433: ScrapyDeprecationWarning: myproject.spiders.quotes_spider.QuotesSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html\n",
            "  warn(\n",
            "2025-07-23 15:41:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2025-07-23 15:41:12 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2025-07-23 15:41:12 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)\n",
            "2025-07-23 15:41:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)\n",
            "2025-07-23 15:41:13 [quotes_spider] DEBUG: Saved file quotes-1.html\n",
            "2025-07-23 15:41:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)\n",
            "2025-07-23 15:41:14 [quotes_spider] DEBUG: Saved file quotes-2.html\n",
            "2025-07-23 15:41:14 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2025-07-23 15:41:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 714,\n",
            " 'downloader/request_count': 3,\n",
            " 'downloader/request_method_count/GET': 3,\n",
            " 'downloader/response_bytes': 5343,\n",
            " 'downloader/response_count': 3,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'downloader/response_status_count/404': 1,\n",
            " 'elapsed_time_seconds': 2.506841,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2025, 7, 23, 15, 41, 14, 705114, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 25016,\n",
            " 'httpcompression/response_count': 3,\n",
            " 'items_per_minute': 0.0,\n",
            " 'log_count/DEBUG': 8,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 306860032,\n",
            " 'memusage/startup': 306860032,\n",
            " 'response_received_count': 3,\n",
            " 'responses_per_minute': 90.0,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/404': 1,\n",
            " 'scheduler/dequeued': 2,\n",
            " 'scheduler/dequeued/memory': 2,\n",
            " 'scheduler/enqueued': 2,\n",
            " 'scheduler/enqueued/memory': 2,\n",
            " 'start_time': datetime.datetime(2025, 7, 23, 15, 41, 12, 198273, tzinfo=datetime.timezone.utc)}\n",
            "2025-07-23 15:41:14 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_code = '''import scrapy\n",
        "\n",
        "class QuotesSpider(scrapy.Spider):\n",
        "    name = \"quotes_code\"\n",
        "\n",
        "    def start_requests(self):\n",
        "        urls = [\n",
        "            'http://quotes.toscrape.com/page/1/',\n",
        "            'http://quotes.toscrape.com/page/2/',\n",
        "        ]\n",
        "        for url in urls:\n",
        "            yield scrapy.Request(url=url, callback=self.parse)\n",
        "\n",
        "    def parse(self, response):\n",
        "        # Select all quote containers\n",
        "        quotes = response.css(\"div.quote\")\n",
        "\n",
        "        # Extract and yield only the quote text\n",
        "        for quote in quotes:\n",
        "            yield {\n",
        "                'quote': quote.css(\"span.text::text\").get()\n",
        "            }\n",
        "'''\n",
        "with open('myproject/myproject/spiders/quotes_code.py', 'w') as f:\n",
        "    f.write(quotes_code)"
      ],
      "metadata": {
        "id": "Ub9CV9s78Woh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd myproject && scrapy crawl quotes_code -o quotes.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fj2PiDazMsmw",
        "outputId": "5f3ccc57-3e58-4096-addc-3c1cd56bbab4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-23 16:55:59 [scrapy.utils.log] INFO: Scrapy 2.13.3 started (bot: myproject)\n",
            "2025-07-23 16:55:59 [scrapy.utils.log] INFO: Versions:\n",
            "{'lxml': '5.4.0',\n",
            " 'libxml2': '2.13.8',\n",
            " 'cssselect': '1.3.0',\n",
            " 'parsel': '1.10.0',\n",
            " 'w3lib': '2.3.1',\n",
            " 'Twisted': '25.5.0',\n",
            " 'Python': '3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]',\n",
            " 'pyOpenSSL': '24.2.1 (OpenSSL 3.3.2 3 Sep 2024)',\n",
            " 'cryptography': '43.0.3',\n",
            " 'Platform': 'Linux-6.1.123+-x86_64-with-glibc2.35'}\n",
            "2025-07-23 16:55:59 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "2025-07-23 16:55:59 [asyncio] DEBUG: Using selector: EpollSelector\n",
            "2025-07-23 16:55:59 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
            "2025-07-23 16:55:59 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
            "2025-07-23 16:55:59 [scrapy.extensions.telnet] INFO: Telnet Password: 2209548a428555d5\n",
            "2025-07-23 16:55:59 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.feedexport.FeedExporter',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2025-07-23 16:55:59 [scrapy.crawler] INFO: Overridden settings:\n",
            "{'BOT_NAME': 'myproject',\n",
            " 'CONCURRENT_REQUESTS_PER_DOMAIN': 1,\n",
            " 'DOWNLOAD_DELAY': 1,\n",
            " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
            " 'NEWSPIDER_MODULE': 'myproject.spiders',\n",
            " 'ROBOTSTXT_OBEY': True,\n",
            " 'SPIDER_MODULES': ['myproject.spiders']}\n",
            "2025-07-23 16:55:59 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2025-07-23 16:55:59 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.start.StartSpiderMiddleware',\n",
            " 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2025-07-23 16:55:59 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2025-07-23 16:55:59 [scrapy.core.engine] INFO: Spider opened\n",
            "/usr/local/lib/python3.11/dist-packages/scrapy/core/spidermw.py:433: ScrapyDeprecationWarning: myproject.spiders.quotes_code.QuotesSpider defines the deprecated start_requests() method. start_requests() has been deprecated in favor of a new method, start(), to support asynchronous code execution. start_requests() will stop being called in a future version of Scrapy. If you use Scrapy 2.13 or higher only, replace start_requests() with start(); note that start() is a coroutine (async def). If you need to maintain compatibility with lower Scrapy versions, when overriding start_requests() in a spider class, override start() as well; you can use super() to reuse the inherited start() implementation without copy-pasting. See the release notes of Scrapy 2.13 for details: https://docs.scrapy.org/en/2.13/news.html\n",
            "  warn(\n",
            "2025-07-23 16:55:59 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2025-07-23 16:55:59 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2025-07-23 16:55:59 [scrapy.core.engine] DEBUG: Crawled (404) <GET http://quotes.toscrape.com/robots.txt> (referer: None)\n",
            "2025-07-23 16:56:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/1/> (referer: None)\n",
            "2025-07-23 16:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
            "{'quote': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'}\n",
            "2025-07-23 16:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
            "{'quote': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”'}\n",
            "2025-07-23 16:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
            "{'quote': '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”'}\n",
            "2025-07-23 16:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
            "{'quote': '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”'}\n",
            "2025-07-23 16:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
            "{'quote': \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\"}\n",
            "2025-07-23 16:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
            "{'quote': '“Try not to become a man of success. Rather become a man of value.”'}\n",
            "2025-07-23 16:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
            "{'quote': '“It is better to be hated for what you are than to be loved for what you are not.”'}\n",
            "2025-07-23 16:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
            "{'quote': \"“I have not failed. I've just found 10,000 ways that won't work.”\"}\n",
            "2025-07-23 16:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
            "{'quote': \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\"}\n",
            "2025-07-23 16:56:01 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
            "{'quote': '“A day without sunshine is like, you know, night.”'}\n",
            "2025-07-23 16:56:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://quotes.toscrape.com/page/2/> (referer: None)\n",
            "2025-07-23 16:56:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>\n",
            "{'quote': \"“This life is what you make it. No matter what, you're going to mess up sometimes, it's a universal truth. But the good part is you get to decide how you're going to mess it up. Girls will be your friends - they'll act like it anyway. But just remember, some come, some go. The ones that stay with you through everything - they're your true best friends. Don't let go of them. Also remember, sisters make the best friends in the world. As for lovers, well, they'll come and go too. And baby, I hate to say it, most of them - actually pretty much all of them are going to break your heart, but you can't give up because if you give up, you'll never find your soulmate. You'll never find that half who makes you whole and that goes for everything. Just because you fail once, doesn't mean you're gonna fail at everything. Keep trying, hold on, and always, always, always believe in yourself, because if you don't, then who will, sweetie? So keep your head high, keep your chin up, and most importantly, keep smiling, because life's a beautiful thing and there's so much to smile about.”\"}\n",
            "2025-07-23 16:56:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>\n",
            "{'quote': '“It takes a great deal of bravery to stand up to our enemies, but just as much to stand up to our friends.”'}\n",
            "2025-07-23 16:56:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>\n",
            "{'quote': \"“If you can't explain it to a six year old, you don't understand it yourself.”\"}\n",
            "2025-07-23 16:56:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>\n",
            "{'quote': \"“You may not be her first, her last, or her only. She loved before she may love again. But if she loves you now, what else matters? She's not perfect—you aren't either, and the two of you may never be perfect together but if she can make you laugh, cause you to think twice, and admit to being human and making mistakes, hold onto her and give her the most you can. She may not be thinking about you every second of the day, but she will give you a part of her that she knows you can break—her heart. So don't hurt her, don't change her, don't analyze and don't expect more than she can give. Smile when she makes you happy, let her know when she makes you mad, and miss her when she's not there.”\"}\n",
            "2025-07-23 16:56:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>\n",
            "{'quote': '“I like nonsense, it wakes up the brain cells. Fantasy is a necessary ingredient in living.”'}\n",
            "2025-07-23 16:56:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>\n",
            "{'quote': '“I may not have gone where I intended to go, but I think I have ended up where I needed to be.”'}\n",
            "2025-07-23 16:56:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>\n",
            "{'quote': \"“The opposite of love is not hate, it's indifference. The opposite of art is not ugliness, it's indifference. The opposite of faith is not heresy, it's indifference. And the opposite of life is not death, it's indifference.”\"}\n",
            "2025-07-23 16:56:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>\n",
            "{'quote': '“It is not a lack of love, but a lack of friendship that makes unhappy marriages.”'}\n",
            "2025-07-23 16:56:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>\n",
            "{'quote': '“Good friends, good books, and a sleepy conscience: this is the ideal life.”'}\n",
            "2025-07-23 16:56:02 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/2/>\n",
            "{'quote': '“Life is what happens to us while we are making other plans.”'}\n",
            "2025-07-23 16:56:02 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2025-07-23 16:56:02 [scrapy.extensions.feedexport] INFO: Stored json feed (20 items) in: quotes.json\n",
            "2025-07-23 16:56:02 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 714,\n",
            " 'downloader/request_count': 3,\n",
            " 'downloader/request_method_count/GET': 3,\n",
            " 'downloader/response_bytes': 5349,\n",
            " 'downloader/response_count': 3,\n",
            " 'downloader/response_status_count/200': 2,\n",
            " 'downloader/response_status_count/404': 1,\n",
            " 'elapsed_time_seconds': 2.705442,\n",
            " 'feedexport/success_count/FileFeedStorage': 1,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2025, 7, 23, 16, 56, 2, 529138, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 25016,\n",
            " 'httpcompression/response_count': 3,\n",
            " 'item_scraped_count': 20,\n",
            " 'items_per_minute': 600.0,\n",
            " 'log_count/DEBUG': 26,\n",
            " 'log_count/INFO': 11,\n",
            " 'memusage/max': 307126272,\n",
            " 'memusage/startup': 307126272,\n",
            " 'response_received_count': 3,\n",
            " 'responses_per_minute': 90.0,\n",
            " 'robotstxt/request_count': 1,\n",
            " 'robotstxt/response_count': 1,\n",
            " 'robotstxt/response_status_count/404': 1,\n",
            " 'scheduler/dequeued': 2,\n",
            " 'scheduler/dequeued/memory': 2,\n",
            " 'scheduler/enqueued': 2,\n",
            " 'scheduler/enqueued/memory': 2,\n",
            " 'start_time': datetime.datetime(2025, 7, 23, 16, 55, 59, 823696, tzinfo=datetime.timezone.utc)}\n",
            "2025-07-23 16:56:02 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bwTihpV_Myfv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}